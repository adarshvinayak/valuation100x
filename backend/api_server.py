#!/usr/bin/env python3
"""
FastAPI Server for DeepResearch Comprehensive Analysis
Provides REST API endpoints and WebSocket connections for the frontend.
Updated: Datetime serialization fixes applied
"""

import asyncio
import json
import logging
import os
import uuid
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, Any, Optional, List
from contextlib import asynccontextmanager

import uvicorn
from fastapi import FastAPI, HTTPException, WebSocket, WebSocketDisconnect, BackgroundTasks, Depends
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, StreamingResponse
from pydantic import BaseModel, Field
import redis.asyncio as redis

# Import your existing analysis components
from run_enhanced_analysis import EnhancedAnalysisRunner
from tools.fmp import get_financials_fmp
from database.supabase_client import supabase_manager

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Global variables
analysis_tasks: Dict[str, Dict[str, Any]] = {}
websocket_connections: Dict[str, List[WebSocket]] = {}
redis_client = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan management"""
    global redis_client
    
    # Startup
    logger.info("Starting DeepResearch API Server...")
    
    # Initialize Supabase
    try:
        await supabase_manager.initialize()
        logger.info("âœ… Supabase connection established")
    except Exception as e:
        logger.error(f"âŒ Supabase initialization failed: {e}")
        # Continue without Supabase - will fallback to local storage
    
    # Initialize Redis for caching and session management
    redis_url = os.getenv("REDIS_URL", "redis://localhost:6379")
    try:
        redis_client = await redis.from_url(redis_url, decode_responses=True)
        await redis_client.ping()
        logger.info("âœ… Redis connection established")
    except Exception as e:
        logger.info(f"ðŸ”„ Redis not available ({redis_url}). Using in-memory storage (normal for Railway free tier).")
        redis_client = None
    
    yield
    
    # Shutdown
    logger.info("Shutting down API server...")
    if redis_client:
        await redis_client.close()
    await supabase_manager.close()

# Initialize FastAPI app
app = FastAPI(
    title="DeepResearch Comprehensive Analysis API",
    description="Professional stock analysis API with real-time progress tracking",
    version="1.0.0",
    lifespan=lifespan
)

# CORS middleware for frontend - Updated for Lovable integration
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allow all origins - can be restricted later for production
    allow_credentials=False,  # Set to False when using allow_origins=["*"]
    allow_methods=["GET", "POST", "PUT", "DELETE", "OPTIONS"],
    allow_headers=["*"],
    expose_headers=["*"],
)

# Pydantic models
class AnalysisRequest(BaseModel):
    ticker: str = Field(..., min_length=1, max_length=10, description="Stock ticker symbol")
    company_name: Optional[str] = Field(None, description="Optional company name")

class AnalysisResponse(BaseModel):
    analysis_id: str
    ticker: str
    company_name: Optional[str]
    status: str
    estimated_duration: str
    websocket_url: str
    created_at: datetime

class AnalysisStatus(BaseModel):
    analysis_id: str
    ticker: str
    status: str
    progress: int
    current_step: str
    current_component: Optional[str]
    estimated_completion: Optional[datetime]
    steps_completed: List[str]
    steps_remaining: List[str]
    error: Optional[str]
    started_at: datetime
    completed_at: Optional[datetime]

class TickerValidation(BaseModel):
    ticker: str
    is_valid: bool
    company_name: Optional[str]
    exchange: Optional[str]
    sector: Optional[str]
    market_cap: Optional[float]
    last_updated: datetime

class SystemHealth(BaseModel):
    status: str
    timestamp: datetime
    version: str
    services: Dict[str, Any]
    active_analyses: int
    queue_size: int

# WebSocket Connection Manager
class ConnectionManager:
    def __init__(self):
        self.active_connections: Dict[str, List[WebSocket]] = {}

    async def connect(self, websocket: WebSocket, analysis_id: str):
        await websocket.accept()
        if analysis_id not in self.active_connections:
            self.active_connections[analysis_id] = []
        self.active_connections[analysis_id].append(websocket)
        logger.info(f"WebSocket connected for analysis {analysis_id}")

    def disconnect(self, websocket: WebSocket, analysis_id: str):
        if analysis_id in self.active_connections:
            self.active_connections[analysis_id].remove(websocket)
            if not self.active_connections[analysis_id]:
                del self.active_connections[analysis_id]
        logger.info(f"WebSocket disconnected for analysis {analysis_id}")

    async def send_to_analysis(self, analysis_id: str, message: dict):
        if analysis_id in self.active_connections:
            disconnected = []
            for connection in self.active_connections[analysis_id]:
                try:
                    await connection.send_json(message)
                except:
                    disconnected.append(connection)
            
            # Remove disconnected connections
            for conn in disconnected:
                self.disconnect(conn, analysis_id)

    async def broadcast_system_status(self, message: dict):
        """Broadcast system-wide messages to all connections"""
        for analysis_id, connections in self.active_connections.items():
            await self.send_to_analysis(analysis_id, message)

manager = ConnectionManager()

# Analysis step definitions
ANALYSIS_STEPS = [
    {"id": "question_generation", "name": "Question Generation", "duration": 60},
    {"id": "sec_filing_analysis", "name": "SEC Filing Analysis", "duration": 180},
    {"id": "financial_data_collection", "name": "Financial Data Collection", "duration": 120},
    {"id": "comprehensive_research", "name": "Comprehensive Research", "duration": 300},
    {"id": "damodaran_story_development", "name": "Damodaran Story Development", "duration": 120},
    {"id": "valuation_scenario_analysis", "name": "Valuation & Scenario Analysis", "duration": 90},
    {"id": "report_generation", "name": "Report Generation", "duration": 30}
]

# Helper Functions
async def validate_ticker(ticker: str) -> TickerValidation:
    """Validate ticker symbol and get company information"""
    try:
        # Use your existing FMP tool to validate
        ticker = ticker.upper().strip()
        
        # Basic ticker format validation
        if not ticker.isalpha() or len(ticker) > 10:
            return TickerValidation(
                ticker=ticker,
                is_valid=False,
                company_name=None,
                exchange=None,
                sector=None,
                market_cap=None,
                last_updated=datetime.utcnow()
            )
        
        # For demonstration - in real implementation, use FMP API
        # This would call your existing tools to validate
        # financials = await get_financials_fmp(ticker)
        
        # Mock validation for now
        mock_companies = {
            "AAPL": {"name": "Apple Inc.", "exchange": "NASDAQ", "sector": "Technology", "market_cap": 2800000000000},
            "MSFT": {"name": "Microsoft Corporation", "exchange": "NASDAQ", "sector": "Technology", "market_cap": 2400000000000},
            "TSLA": {"name": "Tesla Inc.", "exchange": "NASDAQ", "sector": "Automotive", "market_cap": 800000000000},
            "IBM": {"name": "International Business Machines", "exchange": "NYSE", "sector": "Technology", "market_cap": 120000000000}
        }
        
        if ticker in mock_companies:
            company = mock_companies[ticker]
            return TickerValidation(
                ticker=ticker,
                is_valid=True,
                company_name=company["name"],
                exchange=company["exchange"],
                sector=company["sector"],
                market_cap=company["market_cap"],
                last_updated=datetime.utcnow()
            )
        else:
            return TickerValidation(
                ticker=ticker,
                is_valid=False,
                company_name=None,
                exchange=None,
                sector=None,
                market_cap=None,
                last_updated=datetime.utcnow()
            )
            
    except Exception as e:
        logger.error(f"Ticker validation error: {e}")
        raise HTTPException(status_code=500, detail="Ticker validation failed")

async def store_analysis_state(analysis_id: str, state: dict):
    """Store analysis state in Supabase, Redis, and memory"""
    
    # Store in Supabase database
    if supabase_manager.initialized:
        try:
            # Prepare analysis data for Supabase
            analysis_data = {
                'id': analysis_id,
                'ticker': state.get('ticker'),
                'analysis_type': 'comprehensive',
                'status': state.get('status'),
                'session_id': state.get('session_id'),
                'results_json': state,
                'processing_time_seconds': None,
                'created_at': state.get('started_at', datetime.utcnow()).isoformat(),
                'updated_at': datetime.utcnow().isoformat()
            }
            
            # Store or update in Supabase
            if state.get('status') == 'running' and 'started_at' in state:
                # First time - insert
                await supabase_manager.store_analysis_result(analysis_data)
            else:
                # Update existing
                await supabase_manager.update_analysis_status(
                    analysis_id, 
                    state.get('status', 'running'),
                    {
                        'results_json': state,
                        'processing_time_seconds': state.get('processing_time_seconds'),
                        'error_message': state.get('error')
                    }
                )
        except Exception as e:
            logger.error(f"Supabase storage error: {e}")
    
    # Store in Redis for fast access
    if redis_client:
        try:
            await redis_client.setex(f"analysis:{analysis_id}", 3600, json.dumps(state, default=str))
        except Exception as e:
            logger.error(f"Redis storage error: {e}")
    
    # Always store in memory as fallback
    analysis_tasks[analysis_id] = state

async def get_analysis_state(analysis_id: str) -> Optional[dict]:
    """Retrieve analysis state from Redis or memory"""
    if redis_client:
        try:
            data = await redis_client.get(f"analysis:{analysis_id}")
            if data:
                return json.loads(data)
        except Exception as e:
            logger.error(f"Redis retrieval error: {e}")
    
    # Fallback to memory
    return analysis_tasks.get(analysis_id)

# API Endpoints

@app.options("/{path:path}")
async def options_handler(path: str):
    """Handle preflight OPTIONS requests for CORS"""
    return {}

@app.get("/", tags=["Root"])
async def root():
    """Root endpoint"""
    return {
        "message": "DeepResearch Comprehensive Analysis API",
        "version": "1.0.0",
        "documentation": "/docs"
    }

@app.get("/api/health", response_model=SystemHealth, tags=["System"])
async def health_check():
    """System health check"""
    return SystemHealth(
        status="healthy",
        timestamp=datetime.utcnow(),
        version="1.0.0",
        services={
            "redis": "healthy" if redis_client else "unavailable",
            "external_apis": {
                "fmp": "healthy",
                "alpha_vantage": "healthy",
                "sec_api": "healthy",
                "valueinvesting_io": "healthy"
            }
        },
        active_analyses=len(analysis_tasks),
        queue_size=0
    )

@app.get("/api/validate/ticker/{ticker}", response_model=TickerValidation, tags=["Validation"])
async def validate_ticker_endpoint(ticker: str):
    """Validate a stock ticker and get company information"""
    return await validate_ticker(ticker)

@app.post("/api/analysis/comprehensive/start", response_model=AnalysisResponse, tags=["Analysis"])
async def start_comprehensive_analysis(
    request: AnalysisRequest,
    background_tasks: BackgroundTasks
):
    """Start a comprehensive analysis for a stock ticker"""
    
    # Validate ticker first
    validation = await validate_ticker(request.ticker)
    if not validation.is_valid:
        raise HTTPException(
            status_code=400, 
            detail=f"Invalid ticker symbol: {request.ticker}"
        )
    
    # Check if analysis is already running for this ticker
    for analysis_id, task in analysis_tasks.items():
        if task.get("ticker") == request.ticker and task.get("status") == "running":
            raise HTTPException(
                status_code=409,
                detail=f"Analysis already in progress for {request.ticker}"
            )
    
    # Generate analysis ID
    analysis_id = str(uuid.uuid4())
    
    # Initialize analysis state
    analysis_state = {
        "analysis_id": analysis_id,
        "ticker": request.ticker,
        "company_name": request.company_name or validation.company_name,
        "status": "running",
        "progress": 0,
        "current_step": "initializing",
        "current_component": None,
        "steps_completed": [],
        "steps_remaining": [step["id"] for step in ANALYSIS_STEPS],
        "error": None,
        "started_at": datetime.utcnow(),
        "completed_at": None,
        "results": None
    }
    
    # Store initial state
    await store_analysis_state(analysis_id, analysis_state)
    
    # Start analysis in background
    background_tasks.add_task(run_comprehensive_analysis, analysis_id, request.ticker, request.company_name)
    
    return AnalysisResponse(
        analysis_id=analysis_id,
        ticker=request.ticker,
        company_name=request.company_name or validation.company_name,
        status="started",
        estimated_duration="15 minutes",
        websocket_url=f"/ws/analysis/{analysis_id}",
        created_at=datetime.utcnow()
    )

@app.get("/api/analysis/{analysis_id}/status", response_model=AnalysisStatus, tags=["Analysis"])
async def get_analysis_status(analysis_id: str):
    """Get current status of a comprehensive analysis"""
    
    state = await get_analysis_state(analysis_id)
    if not state:
        raise HTTPException(status_code=404, detail="Analysis not found")
    
    # Calculate estimated completion
    estimated_completion = None
    if state["status"] == "running" and state["progress"] > 0:
        elapsed = (datetime.utcnow() - datetime.fromisoformat(state["started_at"].replace('Z', '+00:00'))).total_seconds()
        total_estimated = elapsed / (state["progress"] / 100)
        remaining = total_estimated - elapsed
        estimated_completion = datetime.utcnow() + timedelta(seconds=remaining)
    
    return AnalysisStatus(
        analysis_id=state["analysis_id"],
        ticker=state["ticker"],
        status=state["status"],
        progress=state["progress"],
        current_step=state["current_step"],
        current_component=state.get("current_component"),
        estimated_completion=estimated_completion,
        steps_completed=state["steps_completed"],
        steps_remaining=state["steps_remaining"],
        error=state.get("error"),
        started_at=state["started_at"],
        completed_at=state.get("completed_at")
    )

@app.delete("/api/analysis/{analysis_id}/cancel", tags=["Analysis"])
async def cancel_analysis(analysis_id: str):
    """Cancel a running comprehensive analysis"""
    
    state = await get_analysis_state(analysis_id)
    if not state:
        raise HTTPException(status_code=404, detail="Analysis not found")
    
    if state["status"] != "running":
        raise HTTPException(status_code=400, detail="Analysis is not running")
    
    # Update state to cancelled
    state["status"] = "cancelled"
    state["completed_at"] = datetime.utcnow()
    await store_analysis_state(analysis_id, state)
    
    # Notify WebSocket clients
    await manager.send_to_analysis(analysis_id, {
        "type": "analysis_cancelled",
        "analysis_id": analysis_id,
        "data": {
            "status": "cancelled",
            "cancelled_at": datetime.utcnow().isoformat()
        }
    })
    
    return {
        "analysis_id": analysis_id,
        "status": "cancelled",
        "message": "Analysis cancelled successfully",
        "cancelled_at": datetime.utcnow()
    }

@app.get("/api/analysis/{analysis_id}/results", tags=["Analysis"])
async def get_analysis_results(analysis_id: str):
    """Get comprehensive analysis results"""
    
    state = await get_analysis_state(analysis_id)
    if not state:
        raise HTTPException(status_code=404, detail="Analysis not found")
    
    if state["status"] != "completed":
        raise HTTPException(status_code=400, detail="Analysis not completed")
    
    return {
        "analysis_id": state["analysis_id"],
        "ticker": state["ticker"],
        "company_name": state["company_name"],
        "status": state["status"],
        "results": state.get("results", {}),
        "metadata": {
            "analysis_duration": f"{(state['completed_at'] - state['started_at']).total_seconds() / 60:.1f} minutes",
            "data_sources": ["SEC-API", "FMP", "Alpha Vantage", "ValueInvesting.io"],
            "model_versions": {
                "damodaran_framework": "v2.0",
                "sentiment_model": "FinBERT-v1.1"
            }
        },
        "completed_at": state["completed_at"]
    }

@app.get("/api/reports/{analysis_id}/markdown", tags=["Reports"])
async def get_markdown_report(analysis_id: str):
    """Get the comprehensive analysis report in markdown format"""
    
    state = await get_analysis_state(analysis_id)
    if not state:
        raise HTTPException(status_code=404, detail="Analysis not found")
    
    if state["status"] != "completed":
        raise HTTPException(status_code=400, detail="Analysis not completed")
    
    # Look for saved markdown report
    ticker = state["ticker"]
    output_dir = Path("data/outputs") / ticker
    
    # Find the most recent comprehensive report
    markdown_files = list(output_dir.glob(f"{ticker}_enhanced_comprehensive_*.md"))
    if not markdown_files:
        raise HTTPException(status_code=404, detail="Markdown report not found")
    
    # Get the most recent file
    latest_file = max(markdown_files, key=lambda f: f.stat().st_mtime)
    
    def generate_markdown():
        with open(latest_file, 'r', encoding='utf-8') as f:
            for line in f:
                yield line
    
    return StreamingResponse(
        generate_markdown(),
        media_type="text/markdown",
        headers={"Content-Disposition": f"inline; filename={latest_file.name}"}
    )

@app.post("/api/reports/{analysis_id}/pdf", tags=["Reports"])
async def generate_pdf_report(analysis_id: str):
    """Generate and download PDF report"""
    
    state = await get_analysis_state(analysis_id)
    if not state:
        raise HTTPException(status_code=404, detail="Analysis not found")
    
    if state["status"] != "completed":
        raise HTTPException(status_code=400, detail="Analysis not completed")
    
    # For now, return a placeholder response
    # In production, you would generate PDF from markdown
    raise HTTPException(status_code=501, detail="PDF generation not implemented yet")

# WebSocket endpoint
@app.websocket("/ws/analysis/{analysis_id}")
async def websocket_endpoint(websocket: WebSocket, analysis_id: str):
    """WebSocket connection for real-time analysis updates"""
    
    await manager.connect(websocket, analysis_id)
    
    try:
        # Send initial connection confirmation
        await websocket.send_json({
            "type": "connection_established",
            "analysis_id": analysis_id,
            "timestamp": datetime.utcnow().isoformat()
        })
        
        # Keep connection alive and handle client messages
        while True:
            try:
                # Wait for messages from client (e.g., subscription confirmations)
                data = await websocket.receive_json()
                
                if data.get("type") == "ping":
                    await websocket.send_json({
                        "type": "pong",
                        "timestamp": datetime.utcnow().isoformat()
                    })
                    
            except asyncio.TimeoutError:
                # Send heartbeat
                await websocket.send_json({
                    "type": "heartbeat",
                    "timestamp": datetime.utcnow().isoformat()
                })
                
    except WebSocketDisconnect:
        manager.disconnect(websocket, analysis_id)
    except Exception as e:
        logger.error(f"WebSocket error: {e}")
        manager.disconnect(websocket, analysis_id)

# Background task for running analysis
async def run_comprehensive_analysis(analysis_id: str, ticker: str, company_name: Optional[str]):
    """Run the comprehensive analysis and update progress via WebSocket"""
    
    try:
        logger.info(f"Starting comprehensive analysis for {ticker} (ID: {analysis_id})")
        
        # Initialize the analysis runner
        runner = EnhancedAnalysisRunner()
        
        # Update progress through steps
        for i, step in enumerate(ANALYSIS_STEPS):
            # Update current step
            state = await get_analysis_state(analysis_id)
            if state["status"] == "cancelled":
                logger.info(f"Analysis {analysis_id} was cancelled")
                return
            
            state["current_step"] = step["id"]
            state["progress"] = int((i / len(ANALYSIS_STEPS)) * 100)
            await store_analysis_state(analysis_id, state)
            
            # Send progress update via WebSocket
            await manager.send_to_analysis(analysis_id, {
                "type": "progress_update",
                "analysis_id": analysis_id,
                "data": {
                    "progress": state["progress"],
                    "current_step": step["id"],
                    "current_component": step["name"],
                    "step_description": step["name"],
                    "estimated_completion": (datetime.utcnow() + timedelta(seconds=sum(s["duration"] for s in ANALYSIS_STEPS[i:]))).isoformat(),
                    "timestamp": datetime.utcnow().isoformat()
                }
            })
            
            # Send log message
            await manager.send_to_analysis(analysis_id, {
                "type": "analysis_log",
                "analysis_id": analysis_id,
                "data": {
                    "level": "INFO",
                    "component": step["name"],
                    "message": f"Starting {step['name']} for {ticker}",
                    "timestamp": datetime.utcnow().isoformat(),
                    "metadata": {
                        "step": step["id"],
                        "progress": state["progress"]
                    }
                }
            })
            
            # Simulate step duration (in production, this would be actual analysis)
            await asyncio.sleep(min(step["duration"] / 10, 5))  # Accelerated for demo
            
            # Mark step as completed
            state = await get_analysis_state(analysis_id)
            if state["status"] == "cancelled":
                return
                
            state["steps_completed"].append(step["id"])
            state["steps_remaining"].remove(step["id"])
            await store_analysis_state(analysis_id, state)
            
            # Send step completion
            await manager.send_to_analysis(analysis_id, {
                "type": "step_completed",
                "analysis_id": analysis_id,
                "data": {
                    "completed_step": step["id"],
                    "next_step": ANALYSIS_STEPS[i + 1]["id"] if i + 1 < len(ANALYSIS_STEPS) else None,
                    "progress": state["progress"],
                    "step_duration": f"{step['duration']} seconds",
                    "timestamp": datetime.utcnow().isoformat()
                }
            })
        
        # Run the actual comprehensive analysis
        logger.info(f"Running actual comprehensive analysis for {ticker}")
        
        # Update to show actual analysis running
        await manager.send_to_analysis(analysis_id, {
            "type": "analysis_log",
            "analysis_id": analysis_id,
            "data": {
                "level": "INFO",
                "component": "EnhancedAnalysisRunner",
                "message": f"Executing comprehensive analysis workflow for {ticker}",
                "timestamp": datetime.utcnow().isoformat(),
                "metadata": {
                    "step": "comprehensive_execution",
                    "progress": 95
                }
            }
        })
        
        # Run the actual analysis (this is your existing code)
        results = await runner.run_comprehensive_analysis(ticker, company_name)
        
        # Complete the analysis
        state = await get_analysis_state(analysis_id)
        state["status"] = "completed"
        state["progress"] = 100
        state["current_step"] = "completed"
        state["completed_at"] = datetime.utcnow()
        state["results"] = {
            "investment_score": results.get("base_analysis", {}).get("investment_score", 5.0),
            "fair_value": results.get("base_analysis", {}).get("fair_value", 0.0),
            "current_price": results.get("base_analysis", {}).get("current_price", 0.0),
            "recommendation": "BUY" if results.get("base_analysis", {}).get("investment_score", 5) >= 6 else "HOLD",
            "confidence": 0.85,  # You can extract this from your results
            "analysis_summary": {
                "executive_summary": "Comprehensive analysis completed successfully",
                "key_strengths": ["Strong fundamentals", "Good market position"],
                "key_risks": ["Market volatility", "Sector-specific risks"],
                "price_target": results.get("base_analysis", {}).get("fair_value", 0.0) * 1.1
            }
        }
        
        await store_analysis_state(analysis_id, state)
        
        # Send completion notification
        await manager.send_to_analysis(analysis_id, {
            "type": "analysis_completed",
            "analysis_id": analysis_id,
            "data": {
                "status": "completed",
                "total_duration": f"{(state['completed_at'] - state['started_at']).total_seconds() / 60:.1f} minutes",
                "investment_score": state["results"]["investment_score"],
                "recommendation": state["results"]["recommendation"],
                "results_available": True,
                "report_ready": True,
                "completed_at": state["completed_at"].isoformat()
            }
        })
        
        logger.info(f"Comprehensive analysis completed for {ticker} (ID: {analysis_id})")
        
    except Exception as e:
        logger.error(f"Analysis failed for {ticker} (ID: {analysis_id}): {e}")
        
        # Update state with error
        state = await get_analysis_state(analysis_id)
        if state:
            state["status"] = "failed"
            state["error"] = str(e)
            state["completed_at"] = datetime.utcnow()
            await store_analysis_state(analysis_id, state)
            
            # Send error notification
            await manager.send_to_analysis(analysis_id, {
                "type": "analysis_error",
                "analysis_id": analysis_id,
                "data": {
                    "error_type": "analysis_failure",
                    "error_message": str(e),
                    "failed_step": state.get("current_step", "unknown"),
                    "retry_possible": True,
                    "partial_results_available": False,
                    "timestamp": datetime.utcnow().isoformat()
                }
            })

if __name__ == "__main__":
    # Railway provides PORT environment variable
    port = int(os.getenv("PORT", 8000))
    
    # Detect environment (Railway sets RAILWAY_ENVIRONMENT)
    is_production = os.getenv("RAILWAY_ENVIRONMENT") == "production"
    
    uvicorn.run(
        "api_server:app",
        host="0.0.0.0",
        port=port,
        reload=not is_production,  # No reload in production
        workers=1,  # Single worker for Railway
        log_level="info"
    )

